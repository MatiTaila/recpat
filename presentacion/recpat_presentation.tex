\documentclass[9pt,table]{beamer}

\usepackage[latin1]{inputenc}
%\usepackage[spanish]{babel}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{psfrag}
\usepackage{tabularx}
\usepackage{subfig}
\usepackage{tikz}
\usepackage{multirow}
\usepackage{listings}
\usepackage{wrapfig}

\usepackage{fancyvrb}
\usepackage[framed,numbered,autolinebreaks,useliterate]{mcode}

%% beamer options
\mode<presentation>{
% \usetheme{JuanLesPins}
\usetheme{Berlin}
\definecolor{verdecito}{rgb}{0.52,0.73,0.25}
\definecolor{verdecito2}{rgb}{0.35,0.55,0.15}

\definecolor{v1}{rgb}{0.0392,    0.5882,    0.3255}
\definecolor{v2}{rgb}{0.4314,    0.5882,    0.4314}
\definecolor{v3}{rgb}{0.3373,    0.5765,    0.5137}

% \useoutertheme{split}
% \usecolortheme{seahorse}
% \usecolortheme[named=verdecito]{structure}
\usefonttheme[onlymath]{serif}
% \setbeamercolor{alerted text}{fg=verdecito2}

\usecolortheme[named=v3]{structure}
\setbeamercolor{alerted text}{fg=v3}

\setbeamertemplate{footline}{}

\setbeamertemplate{headline}
{%
  \begin{beamercolorbox}{section in head/foot}
  \insertsectionnavigationhorizontal{\textwidth}{}{}
  \end{beamercolorbox}%
}

% \tikz\draw[draw=none,top color=black,bottom color=v2] (0,0) rectangle (\paperwidth,0.2);
% \tikz\draw[draw=none,top color=black,bottom color=v2!60] (0,0) rectangle (\paperwidth,0.5);

}
%\logo{\includegraphics[width=5cm]{jafi.eps}}

\setbeamercovered{transparent}

\author{José Luis Nunes\\Matías Tailanián}
\institute[U. de la República, Montevideo]{Instituto de Ingeniería Eléctrica \\ Universidad de la República, Montevideo, Uruguay}

\title[Introducción al reconocimiento de patrones 2013]{Introducción al reconocimiento de patrones 2013}
\date[]{}

%% Table of contents en cada Section
\AtBeginSection[]
{
  \begin{frame}<beamer>
    \frametitle{Contents}
    \tableofcontents[currentsection]
  \end{frame}
}

%%otros comandos
\newcommand{\prob}[1]{\mathbf{P}\left(#1\right)}
\newtheorem{teorema}{Teorema}
\newtheorem{idea}{Idea}

\newcommand{\bcol}{\begin{columns}[T]}
\newcommand{\ecol}{\end{columns}}
\newcommand{\col}[1][0.5]{\column{#1\textwidth}}

\newcommand{\bs}{\boldsymbol}

\usepackage{enumitem}
\setitemize[1]{label=\tiny$\blacksquare$}
\setitemize[2]{label=\tiny$\bullet$}
\setitemize[3]{label=\tiny$\bullet$}

% \newenvironment{itemize*}%
%   {\begin{itemize}%
%     \setlength{\topsep}{0pt}%
%     \setlength{\itemsep}{0pt}%
%     \setlength{\parskip}{0pt}}%
%   {\end{itemize}
% }

\begin{document}

\frame[plain]{\titlepage}

\begin{frame}<beamer>
   \frametitle{Contents}
    \tableofcontents
\end{frame}

% \section{Introduction}
% % ========================= FRAME ===============================
% % ===============================================================
% \begin{frame}
% \frametitle{ASDF}
% 
% \end{frame}


\section{Base de datos}
% ========================= FRAME ===============================
% ===============================================================
\begin{frame}
\frametitle{Base de datos}
Seguimiento realizado durante 9 meses sobre 891 vacas de 7 tambos diferentes.\\[.3cm]
\pause
\bcol
\col
\textcolor{v2}{\textbf{Características Fenotípicas}}
\begin{itemize}
	\item Edad.
	\item Condición corporal.
	\item Cantidad de partos.
	\item Anestro.
	\item Intervalo entre partos.
	\item Secado.
	\item Servicios.
	\item Concentración de progesterona.
	\item Cantidad de grasa en la leche.
	\item Cantidad de leche.
\end{itemize}
\pause
\col
\textcolor{v2}{\textbf{Características Genotípicas}}
\begin{figure}[H]
	\centering 
	\only<3-4>{\includegraphics[width=.8\columnwidth]{./pics/genotipos.png}
	\caption{Determinación del genotipo}}
	\label{fig:genotipo}
\end{figure}
\only<3-4>{Clasificación en 3 clases: ``AA'', ``AB'' y ``BB''.}
\ecol
\vspace{.2cm}
\pause
\begin{block}{Resumen}
base de datos acotada y ``limpia'' con varias características fenotípicas que se quieren correlacionar con los genotipos de cada individuo.
\end{block}
\end{frame}

\section{Primera etapa}
% ========================= FRAME ===============================
% ===============================================================
\begin{frame}[fragile]
\frametitle{Etapa 1}
% \vspace{-10pt}
Se abordará el problema como un trabajo de clasificación, tomando los genotipos como clases.
\begin{itemize}
	\item Selección de características: método wrapper . Evalúa el set de atributos utilizando un esquema de aprendizaje y utiliza validación cruzada.
	\item Calsificadores
	\begin{itemize}
		\item Árbol de decisión C4.5
		\item Naive Bayes
		\item k-NN
	\end{itemize}
\end{itemize}
\pause
% \vspace{5pt}
\textcolor{v2}{\textbf{Resultados}}
\vspace{-5pt}
\begin{table}[H]
\centering
	\begin{tabular}{c|c|c|c|c|c|} 
	\cline{2-6}
	& \multicolumn{1}{c|}{\cellcolor[gray]{0.7} \textbf{Tiempo [s]}}  
	& \multicolumn{1}{c|}{\cellcolor[gray]{0.7} $\mathbf{\sqrt{MSE}}$}
	& \multicolumn{1}{c|}{\cellcolor[gray]{0.7} \textbf{F-Measure}}
	& \multicolumn{1}{c|}{\cellcolor[gray]{0.7} $\bs\kappa$}
	& \multicolumn{1}{c|}{\cellcolor[gray]{0.7} \textbf{Bien [\%]}} \\ \hline
	
	\multicolumn{1}{|c|}{\cellcolor[gray]{0.8} \textbf{C4.5}}   & 0.84 & 0.45 & 0.331 & 0 & \cellcolor[gray]{0.9}49.83  \\ \hline
	\multicolumn{1}{|c|}{\cellcolor[gray]{0.8} \textbf{Bayes}}  & 0.37 & 0.45 & 0.345 & 0.0044 & \cellcolor[gray]{0.9}49.60 \\ \hline
	\multicolumn{1}{|c|}{\cellcolor[gray]{0.8} \textbf{k-NN}}   & 0.37 & 0.53 & 0.402 & 0.0043 & \cellcolor[gray]{0.9}45.23 \\ \hline
	\end{tabular} 
\end{table}
\vspace{-10pt}
\pause
\bcol
\col
\begin{verbatim}
   a   b   c   <-- classified as
   0 309   0 |   a = AA
   0 444   0 |   b = AB
   0 138   0 |   c = BB
\end{verbatim}
\col
\vspace{5pt}
\begin{block}{}
\begin{itemize}
	\item \small Resultado determinístico.
	\item \small El clasificador no funcionó adecuadamente.
\end{itemize}
\end{block}
\ecol

\end{frame}


Se puede observar en la tabla anterior que se obtienen resultados muy similares para los 3 algoritmos. Para C4.5 y Bayes se obtiene un porcentaje de aciertos un poco menor al 50\%, mientras que para k-NN los resultados son un poco inferiores. \\

La medida de error \emph{kappa-statistic} ($\bs\kappa$), es un indicador de la performance del algoritmo que tiene en cuenta las coincidencias por azar. Se calcula como $$\bs\kappa = \frac{P_0-P_e}{1-P_e}$$donde $P_0$ es la proporción de coincidencias observadas y $P_e$ la proporción de coincidencias esperadas en las hipótesis de independencia, es decir, coincidencias por azar. Se puede ver en la tabla \ref{tab:resultados_etapa1} que se obtuvieron valores de $\bs\kappa$ realmente bajísimos, siendo este un indicador más de la mala performance alcanzada por los algoritmos.\\

Por otro lado resulta interesante analizar las matrices de confusión que resultan de estos algoritmos. Para el C4.5 se obtiene la siguiente matriz:
Claramente el resultado obtenido no es el esperado. En este caso clasifica todos los patrones como pertenecientes a la clase ``AB'', y como esta clase representa casi el 50\% de todas las muestras, el porcentaje de aciertos coincide. Este es un resultado determinístico, que más allá del porcentaje de aciertos, significa que el clasificador no funcionó adecuadamente. A su vez, analizando el árbol de decisión se puede ver que tiene una sola hoja.\\

Por otro lado las matrices de confusión para los algoritmos \emph{Naive Bayes} y \emph{k-NN} son las siguientes:
\begin{lstlisting}
   a   b   c   <-- classified as							   Bayes
   6 300   3 |   a = AA
   8 435   1 |   b = AB
   1 136   1 |   c = BB
\end{lstlisting}
\begin{lstlisting}
   a   b   c   <-- classified as								k-NN
  63 233  13 |   a = AA
  81 331  32 |   b = AB
  18 111   9 |   c = BB
\end{lstlisting}
Si bien estos dos casos no se obtuvo un resultado determinístico como con C4.5, igualmente los resultados tienen un fuerte sesgo hacia la clasificación de los patrones como pertenecietes a la clase ``AB''.\\

Para mitigar el fenómeno de la salida determinística (y el sesgo) mencionado, el siguiente paso es atacar el problema del desbalance de clases. Para ello se realiza un sorteo aleatorio de las muestras pertenecientes a las 2 clases mayoritarias, de forma que las 3 clases tengan la misma cantidad de patrones. Los resultados se presentan en la siguiente sección.

\section{Segunda etapa - Clases balanceadas}

Las características seleccionadas son la condición corporal al momento del parto, a los 30, 45 y 90 días post parto, la cantidad de lactancias, la edad, el intervalo entre partos, progesterona y la cantidad de leche. \\

Los resultados de la segunda etapa, balanceando las clases, se muestran en la figura \ref{tab:resultados_etapa2}. 

\begin{table}[H]
\centering
	\begin{tabular}{c|c|c|c|c|c|} 
	\cline{2-6}
	& \multicolumn{1}{c|}{\cellcolor[gray]{0.7} \textbf{Tiempo [s]}}  
	& \multicolumn{1}{c|}{\cellcolor[gray]{0.7} $\mathbf{\sqrt{MSE}}$}
	& \multicolumn{1}{c|}{\cellcolor[gray]{0.7} \textbf{F-Measure}}
	& \multicolumn{1}{c|}{\cellcolor[gray]{0.7} $\bs\kappa$}
	& \multicolumn{1}{c|}{\cellcolor[gray]{0.7} \textbf{Bien Clasif [\%]}} \\ \hline
	
	\multicolumn{1}{|c|}{\cellcolor[gray]{0.8} \textbf{C4.5}}   & 12.57 & 0.54 & 0.39 & 0.080 & \cellcolor[gray]{0.9}38.65  \\ \hline
	\multicolumn{1}{|c|}{\cellcolor[gray]{0.8} \textbf{Bayes}}  & 11.77 & 0.50 & 0.30 & 0.044 & \cellcolor[gray]{0.9}36.23 \\ \hline
	\multicolumn{1}{|c|}{\cellcolor[gray]{0.8} \textbf{k-NN}}   & 11.92 & 0.63 & 0.38 & 0.069 & \cellcolor[gray]{0.9}37.92 \\ \hline
	\end{tabular} 
% 	\caption{Resultados etapa 2}
	\label{tab:resultados_etapa2}
\end{table}

Como primer ítem a mencionar se debe destacar el descenso en el porcentaje de aciertos, de algo más del 10\%. Mientas que los resultados de la primera etapa arrojaban un porcentaje de aciertos de aproximadamente 49\%, en esta etapa se nota un descenso hasta alrededor de los 38 puntos pocentuales. Aunque a priori parece un peor resultado, en interesante analizarlo con cuidado ya que por ejemplo el índice $\bs\kappa$ aumentó un órden de magnitud, aunque sigue siendo muy malo.\\

A diferencia de los resultados de la etapa anterior, para el caso del algoritmo C4.5 se obtiene un árbol no trivial con una cantidad total de 147 nodos y 74 hojas. Si bien la clasificación se encuentra por encima de una una clasificación aleatoria, son resultados realmente muy malos. \\

El balanceo de clases logró solucionar el problema del resultado determinístico (o el fuerte sesgo) donde se clasificaba todos los patrones (o casi todos) como pertenecientes a la clase ``AB'', y la matriz de confusión se muestra a continuación:
\vspace{-10pt}
\begin{lstlisting}
  a   b   c   <-- classified as									C4.5   
 44  55  39 |  a = AA
 51  54  33 |  b = AB
 40  36  62 |  c = BB
\end{lstlisting}
\vspace{-1cm}
\begin{lstlisting}
  a   b   c   <-- classified as								   Bayes
 10 102  26 |   a = AA
  6 107  25 |   b = AB
 19  86  33 |   c = BB
\end{lstlisting}
\vspace{-1cm}
\begin{lstlisting}
  a   b   c   <-- classified as									k-NN
 45  52  41 |  a = AA
 54  49  35 |  b = AB
 31  44  63 |  c = BB
\end{lstlisting}

Dado el terriblemente bajo porcentaje de aciertos obtenido en esta etapa, en la siguiente etapa se intentará realizar una extracción de características con métodos más sofisticados e intentar explicar el por qué de los resultados tan malos.

\section{Tercera etapa - extración de características}

En la tercera etapa buscaremos realizar extracción de características con el fin de reducir la dimensionalidad y  buscar características con mayor discriminación. 

Esto tiene como fin reducir los niveles de redundancia entre las características, visualizar características latentes significativas y generar para el futuro una mayor compresión en el proceso de generación de datos. 

\subsection{PCA}

El algoritmo \textbf{PCA} (Análisis de componentes principales) tiene como fin encontrar la base de vectores que mejor exprese la distribución de los datos en el espacio completo. Es similar a encontrar las componentes ortogonales de un vector en un espacio, o lo que es igual, encontrar un conjunto de vectores que combinados en forma lineal representen los elementos. Estos elementos son los vectores propios de la matriz de covarianza correspondiente al espacio original. \textbf{PCA} tiene como fin encontrar un subespacio principal en el cual se maximice la varianza de los datos proyectados. Se ordenan las variables de acuerdo a la cantidad de varianza que concentran y se utilizan solamente las más significativas.
\vspace{-.5cm}
\begin{figure} [H]
\centering
  \subfloat[Características 1-2-3]{\label{fig:pca123} 
  		\includegraphics[width=.5\textwidth]{pics/pca123.pdf}} 
  \subfloat[Características 1-4-3]{\label{fig:pca143} 
  		\includegraphics[width=.5\textwidth]{pics/pca143.pdf}} \\
%   \caption{Datos procesados con el algorítmo PCA} 
  \label{fig:pcas}
\end{figure}

A simple vista resulta muy difícil reconocer algún tipo de estructura sobre los datos, con lo cual es de esperar que la clasificación no entregue mejores resultados de los ya vistos. La distribución de los datos en ambos subespacios (ver figura \ref{pcas}) resulta prácticamente randómica y es imposible identificar visualmente algún cluster por clases. (Esto es medio trucho, pero sincero...)\\

\begin{wrapfigure}{l}{0.6\textwidth}
	\vspace{-25pt}
	\begin{center}
		\includegraphics[width=0.5\textwidth]{pics/varianzaPCA}
	\end{center}
	\vspace{-20pt}
	\caption{Varianza vs Componentes}
	\label{varianzaPCA}
	\vspace{-10pt}
\end{wrapfigure}

En la figura \ref{varianzaPCA} vemos la varianza en función de los componentes, podemos apreciar como la caída es abrupta y tiene sentido trabajar en el espacio de los primeros tres componentes que acumulan la mayor cantidad de varianza.\\

Los resultados de aplicar los clasificadores a los datos procesados con PCA se muestran en la tabla \ref{tab:resultados_PCA}.
\begin{table}[H]
\centering
	\begin{tabular}{c|c|c|c|} 
	\cline{2-4}
	& \multicolumn{1}{c|}{\cellcolor[gray]{0.7} \textbf{Tiempo [s]}}  
	& \multicolumn{1}{c|}{\cellcolor[gray]{0.7} $\bs\kappa$}
	& \multicolumn{1}{c|}{\cellcolor[gray]{0.7} \textbf{Bien Clasif [\%]}} \\ \hline
	
	\multicolumn{1}{|c|}{\cellcolor[gray]{0.8} \textbf{C4.5}}   & 0.03 & 0.143 & \cellcolor[gray]{0.9}42.75  \\ \hline
	\multicolumn{1}{|c|}{\cellcolor[gray]{0.8} \textbf{Bayes}}  & 0.02 & 0.091 & \cellcolor[gray]{0.9}39.37 \\ \hline
	\multicolumn{1}{|c|}{\cellcolor[gray]{0.8} \textbf{k-NN}}   & 0    & 0.149 & \cellcolor[gray]{0.9}43.24 \\ \hline
	\end{tabular} 
% 	\caption{Resultados etapa 2}
	\label{tab:resultados_PCA}
\end{table}
Las matrices de confusión para cada clasificador son:
\vspace{-10pt}
\begin{lstlisting}
  a  b  c   <-- classified as									C4.5
 50 42 46 |  a = 1
 43 59 36 |  b = 2
 39 31 68 |  c = 3
\end{lstlisting}
\vspace{-1cm}
\begin{lstlisting}
  a  b  c   <-- classified as								   Bayes
 19 93 26 |  a = 1
 22 85 31 |  b = 2
 18 61 59 |  c = 3
\end{lstlisting}
\vspace{-1cm}
\begin{lstlisting}
  a  b  c   <-- classified as									k-NN
 58 45 35 |  a = 1
 44 59 35 |  b = 2
 44 32 62 |  c = 3
\end{lstlisting}

Como era de esperarse analizando la distribución de los datos, se obtuvieron resultados muy malos, aunque sensiblemente mejores que en la etapa anterior, llegando en el caso de \emph{k-NN} a un porcentaje de aciertos de un poco más del 43\%.\\

A su vez, la medida estadística $\bs\kappa$ aumentó un orden de magnitud respecto a la etapa anterior, y dos órdenes respecto a la etapa 1.

\subsection{LDA}
El algorítmo LDA (Análisis de discriminantes lineales) tiene como fin seleccionar una proyección que maximice separabilidad inter-clases. Busca una proyección de los datos en un espacio de menor (o igual) dimensión que las iniciales con el fin de que la disciminabilidad inter-clases sea lo más alta posible. Es una técnica supervisada ya que para poder buscar dicha proyección se debe entrenar el sistema con patrones etiquetados.\\

\begin{figure}
	\hspace{-2.5cm}
	\includegraphics[width=1.3\textwidth]{pics/lda_zoom.pdf}
% 	\caption{Proyección de los datos aplicando LDA}
	\label{fig:lda}
\end{figure}

En la figura \ref{fig:lda} se muestran los resultados de aplicar LDA a los datos. Nuevamente resulta imposible obtener algun resultado visualizando la distribución de los datos, pero de todas formas se intenta realizar una clasificación con los algoritmos C4.5, Naive Bayes y k-NN. Estos resultados se muestran en la tabla \ref{tab:resultados_LDA}

\begin{table}[H]
\centering
	\begin{tabular}{c|c|c|c|} 
	\cline{2-4}
	& \multicolumn{1}{c|}{\cellcolor[gray]{0.7} \textbf{Tiempo [s]}}  
	& \multicolumn{1}{c|}{\cellcolor[gray]{0.7} $\bs\kappa$}
	& \multicolumn{1}{c|}{\cellcolor[gray]{0.7} \textbf{Bien Clasif [\%]}} \\ \hline
	
	\multicolumn{1}{|c|}{\cellcolor[gray]{0.8} \textbf{C4.5}}   & 0.03 & 0.1558 & \cellcolor[gray]{0.9} 43.7198  \\ \hline
	\multicolumn{1}{|c|}{\cellcolor[gray]{0.8} \textbf{Bayes}}  & 0 & 0.1667 & \cellcolor[gray]{0.9} 44.4444 \\ \hline
	\multicolumn{1}{|c|}{\cellcolor[gray]{0.8} \textbf{k-NN}}   & 0 & 0.1196 & \cellcolor[gray]{0.9} 41.3043 \\ \hline
	\end{tabular} 
% 	\caption{Resultados etapa 2}
	\label{tab:resultados_LDA}
\end{table}

Como era de esperarse al analizar los patrones en el espacio transformado, nuevamente no es posible realizar una buena clasificación con ninguno de los clasificadores probados.

Las matrices de confusión para cada clasificador son:
\vspace{-10pt}
\begin{lstlisting}
  a  b  c   <-- classified as									C4.5
  7 107 24 |  a = 1
 12 122  4 |  b = 2
  5  81 52 |  c = 3
\end{lstlisting}
\vspace{-1cm}
\begin{lstlisting}
  a  b  c   <-- classified as								   Bayes
 10 85 43 |  a = 1
  7 94 37 |  b = 2
  3 55 80 |  c = 3
\end{lstlisting}
\vspace{-1cm}
\begin{lstlisting}
  a  b  c   <-- classified as									k-NN
 64 48 26 |  a = 1
 66 54 18 |  b = 2
 43 42 53 |  c = 3
\end{lstlisting}

Para corroborar de otra forma que los datos son efectivamente inconcluyentes se presenta la figura \ref{fig:lda_dist}, donde se puede ver con asteriscos los patrones representados en el espacio LDA y con líneas sólidas las distribuciones estimadas por clase. La representación de los patrones se realizó utilizando un color y un nivel distinto para cada clase para ayudar a la claridad de visualización.
\begin{figure}
	\centering
	\includegraphics[width=.8\textwidth]{pics/lda_dist.pdf}
% 	\caption{Estimación de la distribución de los datos}
	\label{fig:lda_dist}
\end{figure}
Se puede corroborar una vez más que las 3 clases son muy dificiles de separar, ya que presentan distribuciones realmente muy similares.

\subsection{Diffusion Maps}
Con el objetivo de probar alguna técnica más sofisticada se utiliza Diffusion Map \cite{bib:diffmap}.\\

Diffusion maps es un algoritmo de \emph{machine learning} que computa una familia de conjuntos de datos en un espacio embebido, usualmente de baja dimensión, cuyas coordenadas pueden ser calculadas de los vectores y valores propios de un operador de difusión de los datos. La distancia euclídea entre puntos en el espacio embebido es la ``distancia de difusión''. A diferencia de otros métodos de reducción de dimensionalidad como PCA, este algoritmo es un método no lineal que se centra en descubrir \emph{manifold} subyacente al muestreo de los datos. Integrando la similitud local de los datos a diferentes escalas, \emph{diffusion maps} da una descripción global de los datos. Es robusto ante perturbaciones ruidosas y computacionalmente barato.\\

Los resultados de pasar a un espacio de 3 dimensiones (para poder visualizarlo) se muestran en la figura \ref{fig:dm}.
\begin{figure}[H]
	\centering 
	\includegraphics[width=.8\textwidth]{./pics/dm.pdf}
% 	\caption{Diffusion Maps}
	\label{fig:dm}
\end{figure}

Como se puede apreciar en la figura, al igual que en los casos anteriores los patrones de las diferentes clases están realmente muy entreverados, no pudiéndose diferenciar ningún cluster.


\section{Conclusiones}
% decir que vamos a seguir intentando porque excede este trabajo. mencionar wombat.

Cuestionarse si un determinado gen puede tener efectos en variables fenotípicas resulta una pregunta ambiciosa incluso para un genetista, y por esa razón es que desde un principio tenemos presente la dificultad del problema al que nos enfrentamos. A diferencia de otro tipo de estudios donde uno trata de elegír descriptores que sospecha que pueden tener una realación con la clasificacón buscada, en nuestro caso buscabamos encontrar la relación entre un tipo de gen y sus marcadores moleculares respecto de un conjunto de variables fenotípicas, sin ningún tipo de conocimiento a priori. En este estudio nuestra parte consistía unicamente en realizar el análisis sobre los valores proporcionados. No tuvimos ninguna participación, como resulta obvio, en el diseño del experimento y la adquisición de datos. Es por eso que llegar a comprender el problema, las variables, los cuestionamientos e inclusive los objetivos, fue una tarea a la cuál se le debió dedicar un tiempo para nada despreciable dentro de los tiempos acotados del proyecto.\\

El problema fue atacado en su mayoria por las herramientas dadas en el curso, utilizando mayoritariamente el software weka y matlab. Se experimentó con una técnica un tanto más sofisticada como lo es ``Diffusion Maps''. A su vez, dado que este trabajo excede los objetivos de este curso ya que forma parte del proyecto de investigación en el que trabajamos, se intentará utilizar otro método para la clasificación: \textbf{Restricted maximum likelihood (REML)}, mediante la implementación en el software \emph{Wombat} \cite{bib:REML}. Es un método prometedor que es muy utilizado en problemas de este tipo.\\

Los resultados fueron contundentes, en la primera etapa, en la cual se trabajo con la base ya depurada pero sin ningun tipo de ajuste, los resultados no fueron los esperados. Todos los clasificadores repondieron en la misma manera comentiendo el mismo error, clasificando a practicamente la totalidad de los patrones en una misma categoría: ``AB''.  Observando la cantidad de patrones por clase, como se muestra en la figura \ref{fig:desbalance_clases}, destaca la categoría ``AB'' superando ampliamente a las restantes. Es probable que eso lleve a que los clasificadores obtengan el mejor resultado (aproximadamente 50\%) clasificando todos los patrones como ``AB''.\\

\begin{figure}[H]
	\centering 
	\includegraphics[width=.75\textwidth]{./pics/desbalance_clases.png}
% 	\caption{}
	\label{fig:desbalance_clases}
\end{figure}

El siguiente paso fue trabajar o ``masajear'' la base de datos con el fin de balancear las clases y normalizar los descriptores. Esto tuvo dos efectos directos, en primer lugar los tres algoritmos de clasificación disminuyeron su performance clasificando en promedio un 37\% de los patrones correctamente. Si bien el resultado es inferor al obtenido previamente, en esta ocación el algoritmo trabajó y desarrollo sus clasificadores como se ve en los resultados de la matriz de confusión.\\

Una vez con la base trabajada y los calsificadores trabajando como es esperable se optó por aplicar los algorítmos básicos de extracción de características, PCA y LDA. En el caso de PCA se optó por trabjar en el subespacio de los primeros cuatro componentes cuaya varianza acumulada supera al 80\%. Este cambio se vio reflejado en un aumento promedio del 5\% en la clasificación, pero más allá de la leve mejora esto nos lleva a pensar que no existe ningun tipo de distribución asociada a las clases. En la figura \ref{fig:pcas} se puede ver como los patrones presentan una distribución casi aleatoria respecto a las clases. Al aplicar LDA se redujo el espacio a una dimensión, la cual no presenta un poder de discriminación suficientemente superior a la observada anteriormente. Los resultados obtenidos son ligeramente superiores a los obtenidos mediante PCA salvo para el clasificador k-NN.\\

Finalmente los resultados demostraron que no es posible expresar, mediante los algorítmos aplicados, una correlación entre el gen estudiado y las variables fenotípicas. Naturalmente no podemos afirmar lo contrario, queda todavía un largo conjunto de herramientas estdísticas por aplicar, algunas de ellas con un enfoque mayor a las dadas en el curso hacia problemas biológicos.\\

También se comprobó como, que en general, cuando uno no puede visualizar la distribución de los datos o incluso las correlaciónes entre las variables medidas de modo visual, previo a cualquier tipo de análisis. Es muy dificil que un algoritmo tenga un buen desempeño, los algoitmos vistos en el curso buscan automatizar ese tipo de análisis que uno puede realizar y hasta sospechar simplemente analizando los datos.














\end{document}


